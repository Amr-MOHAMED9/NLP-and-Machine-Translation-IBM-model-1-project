{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3c55509d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import random \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "acf256b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileEN = open(\"C://Users//Administrator//dat410_europarl//europarl-v7.fr-en.lc.en\",mode='rt', encoding='utf-8')\n",
    "textEN = fileEN.read().strip()\n",
    "fileEN.close()\n",
    "\n",
    "fileFR = open(\"C://Users//Administrator//dat410_europarl//europarl-v7.fr-en.lc.fr\",mode='rt', encoding='utf-8')\n",
    "textFR = fileFR.read().strip()\n",
    "fileFR.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b8998964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = text.replace('&lt;','<')\n",
    "    text = text.replace('&amp;','&')\n",
    "    text = text.replace('&gt;','>')\n",
    "    text = text.replace('&quot;','\"')\n",
    "    text = text.replace('&apos;',\"'\")\n",
    "    text = text.replace(\"\\\\\", \"\")\n",
    "    \n",
    "    return text\n",
    "textEN = clean_text(textEN)\n",
    "textFR = clean_text(textFR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f324e70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "textENlist=textEN.split()\n",
    "textFRlist=textFR.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f7995902",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCounts =Counter(textEN.translate(str.maketrans('', '', string.punctuation)).split())\n",
    "FRCounts =Counter(textFR.translate(str.maketrans('', '', string.punctuation)).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0a00dbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most common english words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the', 19595),\n",
       " ('of', 9497),\n",
       " ('to', 8981),\n",
       " ('and', 7209),\n",
       " ('in', 6158),\n",
       " ('is', 4453),\n",
       " ('that', 4421),\n",
       " ('a', 4385),\n",
       " ('we', 3341),\n",
       " ('this', 3332)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#10 most common english words\n",
    "print('10 most common english words')\n",
    "ENCounts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "854e20b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most common french words\n",
      "[('de', 14520), ('la', 9738), ('et', 6617), ('l', 6511), ('le', 6170), ('les', 5582), ('à', 5498), ('des', 5232), ('que', 4796), ('d', 4555)]\n"
     ]
    }
   ],
   "source": [
    "#10 most common french words\n",
    "print('10 most common french words')\n",
    "print(FRCounts.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e3241997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 19595),\n",
       " ('of', 9497),\n",
       " ('to', 8981),\n",
       " ('and', 7209),\n",
       " ('in', 6158),\n",
       " ('is', 4453),\n",
       " ('that', 4421),\n",
       " ('a', 4385),\n",
       " ('we', 3341),\n",
       " ('this', 3332)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#10 most common english words\n",
    "ENCounts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "38f5fe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "EnwordProbabilites = ENCounts\n",
    "sumofENCounts=sum(ENCounts.values())\n",
    "for item, count in ENCounts.items():\n",
    "    EnwordProbabilites[item] /= sumofENCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b8d40fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of getting the word zerba: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"The probability of getting the word zerba: \"+str(EnwordProbabilites['zebra']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5955bda0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of getting the word speaker: 0.000046241349014\n"
     ]
    }
   ],
   "source": [
    "print(\"The probability of getting the word speaker: \"+\"{:.15f}\".format(EnwordProbabilites['speaker']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0a263d53",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i declare resumed the session of the european parliament adjourned on friday 17 december 1999 , and i would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period .although , as you will have seen , the dreaded ' millennium bug ' failed to materialise , still the people in a number of countries suffered a series of natural disasters that truly were dreadful .you have requested a debate on this subject in the course of the next few days , during this part-session .in the meantime , i should like to observe a minute ' s silence , as a number of members have requested , on behalf of all the victims concerned , particularly those of the terrible storms , in the various countries of the european union .please rise , then , for this minute ' s silence .( the house rose and observed a minute ' s silence )madam president , on a point of order .you will be aware from the press and television that there have been a number of bomb explosions and killin\""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textEN[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0647cdc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'je déclare reprise la session du parlement européen qui avait été interrompue le vendredi 17 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances .comme vous avez pu le constater , le grand \" bogue de l \\' an 2000 \" ne s \\' est pas produit . en revanche , les citoyens d \\' un certain nombre de nos pays ont été victimes de catastrophes naturelles qui ont vraiment été terribles .vous avez souhaité un débat à ce sujet dans les prochains jours , au cours de cette période de session .en attendant , je souhaiterais , comme un certain nombre de collègues me l \\' ont demandé , que nous observions une minute de silence pour toutes les victimes , des tempêtes notamment , dans les différents pays de l \\' union européenne qui ont été touchés .je vous invite à vous lever pour cette minute de silence .( le parlement , debout , observe une minute de silence )madame la présidente , c \\' est une motion de procédure .vous avez probablement appris par la presse'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFR[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1690afbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENsentences = open('C://Users//Administrator//dat410_europarl//europarl-v7.fr-en.lc.en',mode='rt', encoding='utf-8').readlines()\n",
    "FRsentences = open('C://Users//Administrator//dat410_europarl//europarl-v7.fr-en.lc.fr',mode='rt', encoding='utf-8').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6933df26",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempsent=[]\n",
    "for i in ENsentences: \n",
    "    tempsent.append(clean_text(i))\n",
    "ENsentences = tempsent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "18d28f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempsent=[]\n",
    "for i in FRsentences: \n",
    "    tempsent.append(clean_text(i))\n",
    "FRsentences = tempsent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef9b235",
   "metadata": {},
   "source": [
    "# Bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2dd95ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_word(sentence):\n",
    "    strippunct = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "    sentlist = strippunct.split()\n",
    "    if len(sentlist) > 0:\n",
    "        return sentlist[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f7e8cda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_firstwords(listofSentences):\n",
    "    firstwords=[]\n",
    "    for i in listofSentences:\n",
    "        firstwords.append(get_first_word(i))\n",
    "    return firstwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "08cb9b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'although', 'you', 'in', 'please']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_firstwords(ENsentences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ede7626d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def firstwords_probabilities(listofSentences):\n",
    "    firstwords = get_firstwords(listofSentences)\n",
    "    firstwordsCounts = Counter(firstwords)\n",
    "    firstwordsProbabilites = firstwordsCounts\n",
    "    for item, count in firstwordsProbabilites.items():\n",
    "        firstwordsProbabilites[item] /= len(listofSentences)\n",
    "    return firstwordsProbabilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0e8e27b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1141"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstwords_probabilities(ENsentences)['i']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dfbece61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_probabilities(corpus):\n",
    "    strippedpuntcorpus = corpus.translate(str.maketrans('', '', string.punctuation))\n",
    "    WordsCounts = Counter(textEN.translate(str.maketrans('', '', string.punctuation)).split())\n",
    "    return WordsCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "afefc0bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 19595), ('of', 9497), ('to', 8981), ('and', 7209), ('in', 6158)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_probabilities(textEN).most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8fe9a2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_probabilties(listofSentences):\n",
    "    Bigrams = []\n",
    "    for i in listofSentences:\n",
    "        strippunctSentence = i.translate(str.maketrans('', '', string.punctuation))\n",
    "        SentenceTokens = strippunctSentence.split()\n",
    "        for i in range(1,len(SentenceTokens)-1):\n",
    "            Bigrams.append((SentenceTokens[i+1],SentenceTokens[i]))\n",
    "    BigramsCounts = Counter(Bigrams)\n",
    "    BigramsProbabilites = BigramsCounts\n",
    "    sumofBigramCounts=sum(BigramsCounts.values())\n",
    "    for item, count in BigramsProbabilites.items():\n",
    "        BigramsProbabilites[item] /= sumofBigramCounts\n",
    "    return BigramsProbabilites\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6a039f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('the', 'of'), 0.012660605605229544),\n",
       " (('the', 'in'), 0.006657984154415141),\n",
       " (('the', 'to'), 0.004892261710955827),\n",
       " (('commission', 'the'), 0.0042410732920914),\n",
       " (('the', 'that'), 0.003623279151117456),\n",
       " (('the', 'on'), 0.003456307761665039),\n",
       " (('european', 'the'), 0.003439610622719797),\n",
       " (('the', 'for'), 0.0030597507117155476),\n",
       " (('the', 'and'), 0.0029887878711982703),\n",
       " (('be', 'to'), 0.002930347884889924)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_probabilties(ENsentences).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "07e87ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_sentence_probability(sentence,corpus,listofSentences):\n",
    "    firstwordsprobabilities = firstwords_probabilities(listofSentences)\n",
    "    bigramprobabilties = bigram_probabilties(listofSentences)\n",
    "    strippunct = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "    sentlist = strippunct.split()\n",
    "    firstword = get_first_word(sentence)\n",
    "    firstwordprobability = firstwordsprobabilities[firstword]\n",
    "    bigramSentenceProbability=firstwordprobability\n",
    "    for i in range(1,len(sentlist)-1):\n",
    "        bigramSentenceProbability*= bigramprobabilties[(sentlist[i+1],sentlist[i])]\n",
    "    return bigramSentenceProbability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fbf2db1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.00000000000000022407614179756461383590354238486377'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'i declare resumed the session.'\n",
    "\"{:.50f}\".format(bigram_sentence_probability(sentence, textEN,ENsentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e47a368d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.00000000000000023315516742307877319126192896506519'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"{:.50f}\".format(bigram_sentence_probability('je déclare reprise la session',textFR,FRsentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3df6f6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.00000000000000000000000000000000000000000000000000'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"\"\"i declare resumed the session of the\n",
    "european parliament adjourned on friday 17 december 1999 ,\n",
    "and i would like once again to wish you a happy new year\n",
    "in the hope that you enjoyed a pleasant festive period .\n",
    "although , as you will have seen , the dreaded ' millennium bug \n",
    "' failed to materialise , still the people in a number\n",
    "of countries suffered a series of natural disasters that truly were \n",
    "dreadful .you have requested a debate on this subject in the course \n",
    "of the next few days , during this part-session .in the meantime , \n",
    "i should like to observe a minute ' s silence , \n",
    "as a number of members have requested , \n",
    "on behalf of all the victims concerned , particularly those of the \n",
    "terrible storms , in the various countries of the european union .\n",
    "please rise , then , for this minute ' s silence .\n",
    "( the house rose and observed a minute ' s silence )madam president ,\n",
    "on a point of order .you will be aware from the press and television that\n",
    "there have been a number of bomb explosions\"\"\"\n",
    "\n",
    "\"{:.50f}\".format(bigram_sentence_probability(sentence,textEN,ENsentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "001442de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.00000000000000000000000000000000000000000000000000'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"\"\"je déclare reprise la session du parlement européen qui avait été\n",
    "interrompue le vendredi 17 décembre dernier et je vous renouvelle tous\n",
    "mes vux en espérant que vous avez passé de bonnes vacances .\n",
    "comme vous avez pu le constater , le grand \" bogue de l \\' an 2000 \" ne\n",
    "s \\' est pas produit . en revanche , les citoyens d \\' un certain \n",
    "nombre de nos pays ont été victimes de catastrophes naturelles qui\n",
    "ont vraiment été terribles .vous avez souhaité un débat à ce sujet\n",
    "dans les prochains jours , au cours de cette période de session .\n",
    "en attendant , je souhaiterais , comme un certain nombre de collègues\n",
    "me l \\' ont demandé , que nous observions une minute de silence pour\n",
    "toutes les victimes , des tempêtes notamment , dans les différents pays\n",
    "de l \\' union européenne qui ont été touchés .je vous invite à vous\n",
    "lever pour cette minute de silence .( le parlement , debout , observe\n",
    "une minute de silence )madame la présidente , c \\' est une motion de \n",
    "procédure .\"\"\"\n",
    "\n",
    "\"{:.50f}\".format(bigram_sentence_probability(sentence,textFR,FRsentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6011c702",
   "metadata": {},
   "source": [
    "We can see in the cell above, by computing the probabilities for very long sentences, it is very likely that if we are not using a big data source of text/corpus to get a probability of <b>ZERO</b> as the probabilities of the bigrams from which the sentence is formed will be multiplied by each other, and as the corpus isn't big enough ,some of them may be of a probability approximately equal to zero, which will lead at the end the total probability of the sentence to be equal to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c89d462",
   "metadata": {},
   "source": [
    "# Translation models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "65b8cf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "french_stopwords = set(stopwords.words('french'))\n",
    "english_stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d4f36471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_en_word_translation(word,allignments_dict):\n",
    "    word_dict = {}\n",
    "    for key,value in allignments_dict.items():\n",
    "        if key[1] == word:\n",
    "            word_dict[key]=value\n",
    "    word_translation=(sorted(word_dict.items(), key = itemgetter(1), reverse = True))\n",
    "    word_list=[]\n",
    "    if word in english_stopwords:\n",
    "        for key,value in word_translation:\n",
    "            word_list.append(key[0])\n",
    "    else: \n",
    "        for key,value in word_translation:\n",
    "            if key[0] not in french_stopwords:\n",
    "                word_list.append(key[0])\n",
    "        \n",
    "        \n",
    "    return word_list[0]\n",
    "\n",
    "def get_10_en_word_translation(word,allignments_dict):\n",
    "    word_dict = {}\n",
    "    for key,value in allignments_dict.items():\n",
    "        if key[1] == word:\n",
    "            word_dict[key]=value\n",
    "    word_translation=(sorted(word_dict.items(), key = itemgetter(1), reverse = True))\n",
    "    word_list=[]\n",
    "    if word in english_stopwords:\n",
    "        for key,value in word_translation:\n",
    "            word_list.append(key[0])\n",
    "    else: \n",
    "        for key,value in word_translation:\n",
    "            if key[0] not in french_stopwords:\n",
    "                word_list.append(key[0])\n",
    "        \n",
    "        \n",
    "    return word_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e6a77111",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM: iteration: 29\r"
     ]
    }
   ],
   "source": [
    "t={}\n",
    "c_e = {}\n",
    "c_e_f = {}\n",
    "\n",
    "\n",
    "# n of EM iteratons\n",
    "\n",
    "for q in range(30):\n",
    "    print('EM: iteration: '+str(q), end='\\r')\n",
    "    for k in range(len(FRsentences)):\n",
    "        f= FRsentences[k].translate(str.maketrans('', '', string.punctuation)).split()\n",
    "#         f = [word for word in f if word in french_stopwords]\n",
    "        e= ENsentences[k].translate(str.maketrans('', '', string.punctuation)).split()\n",
    "#         e = [word for word in e if word in english_stopwords]\n",
    "        e.insert(0,'NULL')\n",
    "        t_temp = {}\n",
    "        combinations = [(m,str(n)) for n in e for m in f]\n",
    "        for i in f:\n",
    "            for j in e:\n",
    "                if (i,j) not in t.keys():\n",
    "                    t[(i,j)] = random.uniform(0, 1)\n",
    "                if (i,j) not in c_e_f.keys():\n",
    "                    c_e_f[(i,j)] = 0\n",
    "                if j not in c_e.keys():\n",
    "                    c_e[j] = 0\n",
    "                    \n",
    "                t_temp[(i,j)] = t[(i,j)]\n",
    "        #sigma denominator \n",
    "        denom= sum(t_temp.values())\n",
    "        for i in f:\n",
    "            for j in e:\n",
    "                sigma =  t[(i,j)]/ denom\n",
    "                c_e_f[(i,j)] = c_e_f[(i,j)] + sigma\n",
    "                c_e[j] = c_e[j] + sigma\n",
    "        #setting t \n",
    "        for i in f:\n",
    "            for j in e:\n",
    "                t[(i,j)] = c_e_f[(i,j)] / c_e[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bf8f481b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single word translation of the word parliament: parlement\n"
     ]
    }
   ],
   "source": [
    "print('single word translation of the word parliament: '+get_en_word_translation('parliament',t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e8884c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single word translation of the word european: européenne\n"
     ]
    }
   ],
   "source": [
    "print('single word translation of the word european: '+get_en_word_translation('european',t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3dcaf6d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 probable translations of the word european: \n",
      "1- européenne\n",
      "2- basé\n",
      "3- entendre\n",
      "4- partisan\n",
      "5- refusons\n",
      "6- ambitieuse\n",
      "7- décisif\n",
      "8- aérienne\n",
      "9- trouvons\n",
      "10- su\n"
     ]
    }
   ],
   "source": [
    "print('Top 10 probable translations of the word european: ')\n",
    "for i in range(10):\n",
    "    print(str(i+1)+'- ' + get_10_en_word_translation('european',t)[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac0508b",
   "metadata": {},
   "source": [
    "# Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "26ecd594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('commission', 'commission'), 1566),\n",
       " (('européenne', 'european'), 804),\n",
       " (('parlement', 'parliament'), 787),\n",
       " (('union', 'union'), 732),\n",
       " (('monsieur', 'mr'), 717),\n",
       " (('président', 'president'), 589),\n",
       " (('président', 'mr'), 586),\n",
       " (('rapport', 'report'), 570),\n",
       " (('europe', 'europe'), 567),\n",
       " (('monsieur', 'president'), 566)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = []\n",
    "for i in range(len(FRsentences)):\n",
    "    f= FRsentences[i].translate(str.maketrans('', '', string.punctuation)).split()\n",
    "    f = [word for word in f if word not in french_stopwords]\n",
    "    e= ENsentences[i].translate(str.maketrans('', '', string.punctuation)).split()\n",
    "    e = [word for word in e if word not in english_stopwords]\n",
    "    for j in f:\n",
    "        for k in e:\n",
    "            pairs.append((j,k))\n",
    "\n",
    "for i in range(len(FRsentences)):\n",
    "    f= FRsentences[i].translate(str.maketrans('', '', string.punctuation)).split()\n",
    "    f = [word for word in f if word in french_stopwords]\n",
    "    if len(f) < 2:\n",
    "        e= ENsentences[i].translate(str.maketrans('', '', string.punctuation)).split()\n",
    "        e = [word for word in e if word in english_stopwords]\n",
    "        for j in f:\n",
    "            for k in e:\n",
    "                pairs.append((j,k))\n",
    "pairs_counts = Counter(pairs)\n",
    "pairs_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8be34cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: je déclare reprise la session du parlement européen\n",
      "Translation: i parliament recovery the partsession parliament european\n",
      "\n",
      "\n",
      "Sentence: un certain nombre de nos pays ont été victimes de catastrophes naturelles\n",
      "Translation: this number no are countries have victims no disasters\n",
      "\n",
      "\n",
      "Sentence: je invite vous à lever pour cette minute de silence\n",
      "Translation: i commission you please what european minute no silence\n",
      "\n",
      "\n",
      "Sentence: le rapport est donc retiré de l ' ordre du jour \n",
      "Translation: the report is therefore taken no this agenda\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = ['je déclare reprise la session du parlement européen',\n",
    "            'un certain nombre de nos pays ont été victimes de catastrophes naturelles',\n",
    "            'je invite vous à lever pour cette minute de silence',\n",
    "            \"le rapport est donc retiré de l ' ordre du jour \"]\n",
    "translations = []\n",
    "for sentence in sentences:\n",
    "    translation = []\n",
    "    for i in sentence.translate(str.maketrans('', '', string.punctuation)).split():\n",
    "        for k in pairs_counts.most_common():\n",
    "            if k[0][0]==i:\n",
    "                translation.append(k[0][1])\n",
    "                break\n",
    "    finaltranslation=[]\n",
    "    for i in range(0,len(translation)):\n",
    "        if i> 0 and translation[i] != translation[i-1]:\n",
    "            finaltranslation.append(translation[i])\n",
    "        elif i ==0: \n",
    "            finaltranslation.append(translation[i])\n",
    "    translations.append(finaltranslation)\n",
    "for  i in range (len(sentences)):\n",
    "    print('Sentence: '+ sentences[i])\n",
    "    print('Translation: '+\" \".join(translations[i]))\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
